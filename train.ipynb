{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b965d31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Device: CPU\n",
      "üì¶ Batch size: 16\n",
      "üîÑ Epochs: 3\n",
      "üë∑ Workers: 0\n",
      "[train] classes -> ['fake', 'real'] {'fake': 0, 'real': 1}\n",
      "[val] classes -> ['fake', 'real'] {'fake': 0, 'real': 1}\n",
      "[test] classes -> ['fake', 'real'] {'fake': 0, 'real': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\timm\\models\\_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Running on CPU - skipping torch.compile and DataParallel\n",
      "‚ö° Warming up GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_21516\\155281383.py:139: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_21516\\155281383.py:194: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Warmup completed (3 batches)\n",
      "\n",
      "üöÄ Starting Epoch 1/3...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 201\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müöÄ Starting Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 201\u001b[0m tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m run_epoch(train_loader, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    202\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m run_epoch(val_loader,  \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    203\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[1], line 166\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, train_mode)\u001b[0m\n\u001b[0;32m    164\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    167\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    169\u001b[0m loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    649\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[0;32m    355\u001b[0m     tensors,\n\u001b[0;32m    356\u001b[0m     grad_tensors_,\n\u001b[0;32m    357\u001b[0m     retain_graph,\n\u001b[0;32m    358\u001b[0m     create_graph,\n\u001b[0;32m    359\u001b[0m     inputs_tuple,\n\u001b[0;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    362\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ================= Notebook-ready training script =================\n",
    "import os, math, time, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ---------- 1) PARAMS (ƒë·ªïi tr·ª±c ti·∫øp ·ªü ƒë√¢y) ----------\n",
    "DATASET_ROOT   = \"Dataset\"         # c√≥ train/val/test m·ªói c√°i g·ªìm fake/real\n",
    "DATASET_ALIAS  = Path(DATASET_ROOT).name      # d√πng ƒë·ªÉ ƒë·∫∑t t√™n file l∆∞u (ƒë·ªïi t√πy √Ω)\n",
    "MODEL_KEY      = \"efficientnet_b3\" # {\"efficientnet_b3\", \"xception\"}\n",
    "INPUT_SIZE     = 224               # 224 cho c·∫£ 2 model (ok); Xception g·ªëc 299 v·∫´n ch·∫°y 224\n",
    "\n",
    "# CPU/Notebook optimized settings\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE     = 16 if device_type == \"cpu\" else 32      # Gi·∫£m batch size cho CPU\n",
    "EPOCHS         = 3 if device_type == \"cpu\" else 10       # √çt epochs h∆°n cho demo\n",
    "LR             = 1e-4\n",
    "WEIGHT_DECAY   = 1e-4\n",
    "NUM_WORKERS    = 0 if device_type == \"cpu\" else 2        # CPU: 0 workers, GPU: 2 workers  \n",
    "USE_AMP        = device_type == \"cuda\"                   # AMP ch·ªâ cho CUDA\n",
    "SEED           = 42\n",
    "PREFETCH_FACTOR = 2 if device_type == \"cuda\" else None   # Ch·ªâ d√πng khi c√≥ workers\n",
    "DROP_CONNECT   = 0.2               # cho EfficientNetB3\n",
    "DROPOUT        = 0.3               # cho EfficientNetB3\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "print(f\"üíª Device: {device_type.upper()}\")\n",
    "print(f\"üì¶ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"üîÑ Epochs: {EPOCHS}\")\n",
    "print(f\"üë∑ Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# ---------- 2) SETUP ----------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# GPU memory optimization\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üöÄ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# ---------- 3) DATA ----------\n",
    "# Chu·∫©n ImageNet\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(INPUT_SIZE * 1.15)),\n",
    "    transforms.CenterCrop(INPUT_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "def make_loader(split, tfm, shuffle):\n",
    "    p = os.path.join(DATASET_ROOT, split)\n",
    "    ds = datasets.ImageFolder(p, transform=tfm)\n",
    "    # ƒê·∫£m b·∫£o mapping nh√£n ·ªïn ƒë·ªãnh (mong mu·ªën: {'fake':0,'real':1})\n",
    "    print(f\"[{split}] classes ->\", ds.classes, ds.class_to_idx)\n",
    "    \n",
    "    # DataLoader settings based on device and workers\n",
    "    loader_kwargs = {\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"shuffle\": shuffle,\n",
    "        \"num_workers\": NUM_WORKERS,\n",
    "        \"pin_memory\": device_type == \"cuda\",  # Ch·ªâ pin_memory v·ªõi CUDA\n",
    "    }\n",
    "    \n",
    "    # Ch·ªâ th√™m prefetch_factor v√† persistent_workers khi c√≥ workers\n",
    "    if NUM_WORKERS > 0:\n",
    "        loader_kwargs.update({\n",
    "            \"prefetch_factor\": PREFETCH_FACTOR,\n",
    "            \"persistent_workers\": True\n",
    "        })\n",
    "    \n",
    "    return ds, DataLoader(ds, **loader_kwargs)\n",
    "\n",
    "train_ds, train_loader = make_loader(\"train\", train_tfms, True)\n",
    "val_ds,   val_loader   = make_loader(\"val\",   eval_tfms,   False)\n",
    "test_ds,  test_loader  = make_loader(\"test\",  eval_tfms,   False)\n",
    "\n",
    "NUM_CLASSES = len(train_ds.classes)\n",
    "\n",
    "# ---------- 4) MODEL ----------\n",
    "# Import t·ª´ file ng∆∞·ªùi d√πng cung c·∫•p\n",
    "from efficientnet import EfficientNetB3\n",
    "from xception import Xception\n",
    "\n",
    "def build_model(key: str, num_classes: int):\n",
    "    key = key.lower()\n",
    "    if key == \"efficientnet_b3\":\n",
    "        model = EfficientNetB3(num_classes=num_classes,\n",
    "                               drop_connect_rate=DROP_CONNECT,\n",
    "                               dropout=DROPOUT)\n",
    "        name = \"efficientnet_b3\"\n",
    "    elif key == \"xception\":\n",
    "        model = Xception(num_classes=num_classes)\n",
    "        name = \"xception\"\n",
    "    else:\n",
    "        raise ValueError(f\"MODEL_KEY kh√¥ng h·ª£p l·ªá: {key}\")\n",
    "    return model, name\n",
    "\n",
    "model, model_name = build_model(MODEL_KEY, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# PyTorch 2.0+ compile ƒë·ªÉ tƒÉng t·ªëc (ch·ªâ v·ªõi CUDA)\n",
    "if device_type == \"cuda\":\n",
    "    try:\n",
    "        model = torch.compile(model, mode='reduce-overhead')\n",
    "        print(\"‚úÖ Model compiled with torch.compile\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è torch.compile failed (PyTorch < 2.0?): {e}\")\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs (DataParallel).\")\n",
    "        model = nn.DataParallel(model)\n",
    "else:\n",
    "    print(\"üíª Running on CPU - skipping torch.compile and DataParallel\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# ---------- 5) TRAIN / EVAL LOOPS ----------\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device.type == \"cuda\"))\n",
    "\n",
    "def run_epoch(loader, train_mode=True):\n",
    "    model.train(train_mode)\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    progress_interval = max(1, len(loader) // 10)  # Show progress 10 times per epoch\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # non_blocking ch·ªâ c√≥ t√°c d·ª•ng v·ªõi CUDA\n",
    "        non_blocking = device_type == \"cuda\"\n",
    "        x, y = x.to(device, non_blocking=non_blocking), y.to(device, non_blocking=non_blocking)\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            if scaler.is_enabled():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(x)\n",
    "                    loss = criterion(logits, y)\n",
    "            else:\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "        \n",
    "        # Show progress during training\n",
    "        if train_mode and (batch_idx + 1) % progress_interval == 0:\n",
    "            current_acc = correct / total\n",
    "            print(f\"  üìà Batch {batch_idx+1}/{len(loader)} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | Acc: {current_acc:.4f}\")\n",
    "    \n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "# ---------- 6) TRAINING ----------\n",
    "best_val = -1.0\n",
    "ckpt_path = os.path.join(CHECKPOINT_DIR, f\"{model_name}_{DATASET_ALIAS}_best.pth\")\n",
    "\n",
    "# Warmup: ch·∫°y v√†i batch ƒë·∫ßu ƒë·ªÉ \"n√≥ng m√°y\" GPU\n",
    "print(\"‚ö° Warming up GPU...\")\n",
    "model.train()\n",
    "warmup_batches = min(3, len(train_loader))\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    if i >= warmup_batches:\n",
    "        break\n",
    "    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        _ = model(x)\n",
    "print(f\"‚úÖ Warmup completed ({warmup_batches} batches)\")\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nüöÄ Starting Epoch {ep}/{EPOCHS}...\")\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, True)\n",
    "    val_loss, val_acc = run_epoch(val_loader,  False)\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        # L∆∞u duy nh·∫•t best (state_dict ƒë·ªÉ g·ªçn, t∆∞∆°ng th√≠ch)\n",
    "        to_save = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "        torch.save({\"model\": to_save,\n",
    "                    \"epoch\": ep,\n",
    "                    \"val_acc\": best_val,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"dataset_alias\": DATASET_ALIAS,\n",
    "                    \"input_size\": INPUT_SIZE}, ckpt_path)\n",
    "\n",
    "    print(f\"Epoch {ep:02d} | \"\n",
    "          f\"train_loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "          f\"val_loss {val_loss:.4f} acc {val_acc:.4f} | \"\n",
    "          f\"best_val {best_val:.4f} | time {time.time()-t0:.1f}s\")\n",
    "\n",
    "print(f\"\\nBest checkpoint saved to: {ckpt_path}\")\n",
    "\n",
    "# ---------- 7) TEST v·ªõi best ----------\n",
    "if os.path.isfile(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    sd = ckpt[\"model\"]\n",
    "    (model.module if isinstance(model, nn.DataParallel) else model).load_state_dict(sd)\n",
    "\n",
    "test_loss, test_acc = run_epoch(test_loader, False)\n",
    "print(f\"TEST: loss {test_loss:.4f} | acc {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
