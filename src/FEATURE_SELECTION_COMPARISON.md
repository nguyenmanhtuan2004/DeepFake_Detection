# So sÃ¡nh Feature Selection Methods

## Paper Method vs Others

| Method | Description | Pros | Cons | Paper Use |
|--------|-------------|------|------|-----------|
| **Random Forest** | Feature importance tá»« RF trees | - Fast<br>- Handle non-linear<br>- Robust | - Bias vá»›i high cardinality | âœ… YES |
| **XGBoost** | Gradient boosting importance | - Very accurate<br>- Handle imbalance | - Slower training | âœ… YES |
| **Ensemble (RF+XGB)** | Average importance tá»« cáº£ 2 | - **Best of both**<br>- More stable<br>- Reduce bias | - Need train 2 models | âœ…âœ… PAPER |
| Mutual Information | Statistical dependency | - Model-agnostic<br>- Fast | - Linear assumption | âŒ NO |
| ANOVA F-test | Statistical test | - Fast<br>- Interpretable | - Linear only | âŒ NO |

## Táº¡i sao Paper dÃ¹ng RF + XGBoost?

### 1. **Complementary Strengths**
- **RF**: Captures feature interactions through bagging
- **XGBoost**: Captures sequential patterns through boosting
- **Ensemble**: Combines both perspectives

### 2. **Tree-based Importance**
```
Feature Importance = Weighted average cá»§a:
- Sá»‘ láº§n feature Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ split
- Information gain tá»« má»—i split
- Position trong tree (root â†’ leaf)
```

### 3. **Ranking-based Selection**
```python
# RF importance
rf_scores = [0.05, 0.12, 0.08, ...]  # 3584 features

# XGBoost importance  
xgb_scores = [0.07, 0.10, 0.09, ...]

# Ensemble
ensemble_scores = (rf_scores + xgb_scores) / 2

# Select top K
top_k_indices = argsort(ensemble_scores)[-1000:]
```

## Implementation Comparison

### âŒ CÃ¡ch cÅ© (Statistical):
```python
# Mutual Information - khÃ´ng theo paper
selector = SelectKBest(mutual_info_classif, k=1000)
X_selected = selector.fit_transform(X, y)
```

### âœ… CÃ¡ch paper (Tree-based Ensemble):
```python
# Train RF
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

# Train XGBoost
xgb = XGBClassifier(n_estimators=100)
xgb.fit(X_train, y_train)

# Combine importances
importances = (rf.feature_importances_ + xgb.feature_importances_) / 2

# Select top K
indices = argsort(importances)[::-1][:k]
X_selected = X[:, indices]
```

## Expected Results

Paper Ä‘áº¡t Ä‘Æ°á»£c:
- Celeb-DF: **96.33%** accuracy
- FaceForensics++: **98.00%** accuracy

Vá»›i feature selection Ä‘Ãºng method nÃ y, ká»³ vá»ng tÄƒng **1-3%** so vá»›i statistical methods!

## Káº¿t luáº­n

âœ… **CÃ¡ch cá»§a paper hay hÆ¡n** vÃ¬:
1. Tree-based models handle non-linear relationships tá»‘t hÆ¡n
2. Ensemble giáº£m bias vÃ  variance
3. PhÃ¹ há»£p vá»›i deep features (high-dimensional, non-linear)
4. ÄÃ£ Ä‘Æ°á»£c validate vá»›i káº¿t quáº£ cao trong paper

Code hiá»‡n táº¡i Ä‘Ã£ implement Ä‘Ãºng theo paper! ğŸ¯
