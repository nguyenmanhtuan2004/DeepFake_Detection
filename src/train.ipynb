{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5343f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from data_loader import create_dataloaders\n",
    "from feature_extractor_finetuned import extract_and_stack_features_finetuned, save_features, load_features\n",
    "from feature_selector import select_features_ensemble\n",
    "from models import MetaLearnerMLP\n",
    "import os\n",
    "\n",
    "class FinetunedTrainer:\n",
    "    \"\"\"\n",
    "    Train meta-learner using features extracted from FINE-TUNED models\n",
    "    (not frozen pretrained ImageNet models)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, batch_size=32, device='cuda', feature_ratio=0.5,\n",
    "                 xception_ckpt='../baseline/xception_Dataset_best.pth',\n",
    "                 efficientnet_ckpt='../baseline/efficientnet_b3_Dataset_best.pth'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.feature_ratio = feature_ratio\n",
    "        self.xception_ckpt = xception_ckpt\n",
    "        self.efficientnet_ckpt = efficientnet_ckpt\n",
    "        self.feature_dir = 'features_finetuned'\n",
    "        os.makedirs(self.feature_dir, exist_ok=True)\n",
    "        \n",
    "    def extract_train_features(self):\n",
    "        train_path = f'{self.feature_dir}/train_features.npz'\n",
    "        if os.path.exists(train_path):\n",
    "            print(f\"Loading cached features from {train_path}\")\n",
    "            X_train, y_train = load_features(train_path)\n",
    "        else:\n",
    "            train_loader, _, _ = create_dataloaders(self.data_dir, self.batch_size, img_size=224, num_workers=4)\n",
    "            X_train, y_train = extract_and_stack_features_finetuned(\n",
    "                train_loader, self.device, \n",
    "                self.xception_ckpt, self.efficientnet_ckpt\n",
    "            )\n",
    "            save_features(X_train, y_train, train_path)\n",
    "        return X_train, y_train\n",
    "    \n",
    "    def extract_eval_features(self, split):\n",
    "        path = f'{self.feature_dir}/{split}_features.npz'\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Loading cached features from {path}\")\n",
    "            return load_features(path)\n",
    "        _, val_loader, test_loader = create_dataloaders(self.data_dir, self.batch_size, img_size=224, num_workers=4)\n",
    "        loader = val_loader if split == 'val' else test_loader\n",
    "        X, y = extract_and_stack_features_finetuned(\n",
    "            loader, self.device,\n",
    "            self.xception_ckpt, self.efficientnet_ckpt\n",
    "        )\n",
    "        save_features(X, y, path)\n",
    "        return X, y\n",
    "    \n",
    "    def train_meta_learner(self, X_train, y_train, X_val, y_val, epochs=100, lr=0.001, batch_size=512):\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = MetaLearnerMLP(input_dim=input_dim).to(self.device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # SGD with momentum: á»•n Ä‘á»‹nh vÃ  hiá»‡u quáº£ cho classification\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "        \n",
    "        # OneCycleLR: tÄƒng lr lÃªn max rá»“i giáº£m dáº§n (há»™i tá»¥ nhanh)\n",
    "        steps_per_epoch = (len(X_train) + batch_size - 1) // batch_size\n",
    "        total_steps = steps_per_epoch * epochs\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=lr,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=0.3,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader cho mini-batch training\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_train),\n",
    "            torch.LongTensor(y_train)\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n",
    "        y_val_tensor = torch.LongTensor(y_val).to(self.device)\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training vá»›i mini-batches\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # OneCycleLR step má»—i batch\n",
    "                \n",
    "                train_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "            train_loss /= len(X_train)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val_tensor)\n",
    "                val_loss = criterion(val_outputs, y_val_tensor)\n",
    "                val_preds = val_outputs.argmax(dim=1).cpu().numpy()\n",
    "                val_acc = accuracy_score(y_val, val_preds)\n",
    "                \n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "                \n",
    "                if (epoch + 1) % 5 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss.item():.4f} - Val Acc: {val_acc:.4f} - Best: {best_val_acc:.4f}')\n",
    "        \n",
    "        print(f\"\\nâœ… Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "        return model\n",
    "    \n",
    "    def evaluate(self, model, X_test, y_test):\n",
    "        model.eval()\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test_tensor)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "        \n",
    "        thresh=0.7\n",
    "        preds = (probs.cpu().numpy() > thresh).astype(int)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        prec = precision_score(y_test, preds)\n",
    "        rec = recall_score(y_test, preds)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        \n",
    "        print(f'\\nðŸŽ¯ Test Results (Fine-tuned Features):')\n",
    "        print(f'Accuracy:  {acc:.4f}')\n",
    "        print(f'Precision: {prec:.4f}')\n",
    "        print(f'Recall:    {rec:.4f}')\n",
    "        print(f'F1-Score:  {f1:.4f}')\n",
    "        \n",
    "        return acc, prec, rec, f1\n",
    "    \n",
    "    def run(self):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        print(\"\\nStep 1: Extract Train Features from Fine-tuned Models\")\n",
    "        X_train, y_train = self.extract_train_features()\n",
    "        print(f\"Train: {X_train.shape}\")\n",
    "        \n",
    "        print(\"\\nStep 2: Normalize Features\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        \n",
    "        print(\"\\nStep 3: Extract Val Features\")\n",
    "        X_val, y_val = self.extract_eval_features('val')\n",
    "        X_val = scaler.transform(X_val)\n",
    "        print(f\"Val: {X_val.shape}\")\n",
    "        \n",
    "        print(\"\\nStep 4: Feature Selection (XGBoost)\")\n",
    "        from xgboost import XGBClassifier\n",
    "        k_features = int(X_train.shape[1] * self.feature_ratio)\n",
    "        print(f\"Selecting top {k_features} features ({self.feature_ratio*100:.0f}%)\")\n",
    "        xgb = XGBClassifier(\n",
    "            n_estimators=200, \n",
    "            max_depth=6,\n",
    "            learning_rate=0.001,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42, \n",
    "            tree_method='hist', \n",
    "            device='cuda'\n",
    "        )\n",
    "        xgb.fit(X_train, y_train)\n",
    "        indices = np.argsort(xgb.feature_importances_)[::-1][:k_features]\n",
    "        X_train_sel = X_train[:, indices]\n",
    "        X_val_sel = X_val[:, indices]\n",
    "        print(f\"Selected: {X_train_sel.shape}\")\n",
    "        \n",
    "        print(\"\\nStep 5: Train Meta-Learner MLP (SGD + OneCycleLR)\")\n",
    "        model = self.train_meta_learner(X_train_sel, y_train, X_val_sel, y_val, epochs=50, lr=0.0001, batch_size=512)\n",
    "        \n",
    "        # Save ensemble model\n",
    "        print(\"\\nðŸ’¾ Saving ensemble model...\")\n",
    "        save_path = 'ensemble_finetuned_best.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'scaler': scaler,\n",
    "            'selected_indices': indices,\n",
    "            'input_dim': X_train_sel.shape[1],\n",
    "            'feature_ratio': self.feature_ratio,\n",
    "            'xception_ckpt': self.xception_ckpt,\n",
    "            'efficientnet_ckpt': self.efficientnet_ckpt\n",
    "        }, save_path)\n",
    "        print(f\"âœ… Saved to {save_path}\")\n",
    "        \n",
    "        print(\"\\nStep 6: Load ensemble model and Evaluate on Test Set\")\n",
    "        checkpoint = torch.load(save_path, map_location=self.device, weights_only=False)\n",
    "        model = MetaLearnerMLP(input_dim=checkpoint['input_dim']).to(self.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        scaler = checkpoint['scaler']\n",
    "        indices = checkpoint['selected_indices']\n",
    "        print(\"âœ… Model + Scaler + Feature Indices loaded!\")\n",
    "\n",
    "        X_test, y_test = self.extract_eval_features('test')\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_test_sel = X_test[:, indices]\n",
    "        print(f\"Test: {X_test_sel.shape}\")\n",
    "        \n",
    "        # Evaluate vÃ  lÆ°u test_acc vÃ o checkpoint\n",
    "        test_acc, prec, rec, f1 = self.evaluate(model, X_test_sel, y_test)\n",
    "        checkpoint['test_acc'] = test_acc\n",
    "        torch.save(checkpoint, save_path)\n",
    "        print(f\"âœ… Updated {save_path} with test_acc: {test_acc:.4f}\")\n",
    "if __name__ == '__main__':\n",
    "    trainer = FinetunedTrainer(\n",
    "        data_dir='../Dataset',\n",
    "        batch_size=64,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        feature_ratio=0.6,  \n",
    "        xception_ckpt='../baseline/xception_Dataset_best.pth',\n",
    "        efficientnet_ckpt='../baseline/efficientnet_b3_Dataset_best.pth'\n",
    "    )\n",
    "    trainer.run()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
