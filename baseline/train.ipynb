{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965d31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] classes -> ['fake', 'real'] {'fake': 0, 'real': 1}\n",
      "[val] classes -> ['fake', 'real'] {'fake': 0, 'real': 1}\n",
      "[test] classes -> ['fake', 'real'] {'fake': 0, 'real': 1}\n",
      "[EfficientNetB3] ƒêang t·∫°o model 'tf_efficientnet_b3' v·ªõi pretrained=True\n",
      "[EfficientNetB3] ‚úÖ T·∫°o th√†nh c√¥ng!\n",
      "[EfficientNetB3] ‚úÖ Model validation passed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munfreeze\u001b[39m\u001b[38;5;124m\"\u001b[39m): model\u001b[38;5;241m.\u001b[39munfreeze()\n\u001b[0;32m    126\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 127\u001b[0m tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m run_epoch(train_loader, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    128\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m run_epoch(val_loader, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    129\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[4], line 107\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, train_mode)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(train_mode):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, enabled\u001b[38;5;241m=\u001b[39mscaler\u001b[38;5;241m.\u001b[39mis_enabled()):\n\u001b[1;32m--> 107\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(x); loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[0;32m    109\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\ComputerVision\\LYTHUYET\\DeepFake_Detection\\my_efficientnet.py:166\u001b[0m, in \u001b[0;36mEfficientNetB3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# DataParallel-safe forward: tr·ª±c ti·∫øp g·ªçi backbone thay v√¨ check hasattr\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# hasattr c√≥ th·ªÉ fail khi model ƒë∆∞·ª£c replicate sang GPU kh√°c\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(x)\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    169\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEfficientNetB3.backbone kh√¥ng t·ªìn t·∫°i. C√≥ th·ªÉ do:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. L·ªói trong __init__ (backbone ch∆∞a ƒë∆∞·ª£c t·∫°o)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. DataParallel replication issues\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Model ƒë∆∞·ª£c load kh√¥ng ƒë√∫ng c√°ch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\timm\\models\\efficientnet.py:339\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_features(x)\n\u001b[0;32m    340\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\timm\\models\\efficientnet.py:317\u001b[0m, in \u001b[0;36mEfficientNet.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    315\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x, flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n\u001b[0;32m    318\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_head(x)\n\u001b[0;32m    319\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\timm\\models\\_efficientnet_blocks.py:289\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    287\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    288\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_dw(x)\n\u001b[1;32m--> 289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x)\n\u001b[0;32m    290\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maa(x)\n\u001b[0;32m    291\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mse(x)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\timm\\layers\\norm_act.py:148\u001b[0m, in \u001b[0;36mBatchNormAct2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m    137\u001b[0m     x,\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    146\u001b[0m )\n\u001b[0;32m    147\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[1;32m--> 148\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:434\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2374\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   2373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   2375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # ===== FAST Fine-tune: EfficientNetB3 / Xception =====\n",
    "# import os, time, random\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "# from torchvision import datasets, transforms\n",
    "# from torchvision.transforms import InterpolationMode\n",
    "\n",
    "# # ---------- Params ----------\n",
    "# DATASET_ROOT   = \"../Dataset\"                 # Dataset/train, val, test\n",
    "# MODEL_KEY      = \"efficientnet_b3\"         # {\"efficientnet_b3\", \"xception\"}\n",
    "# INPUT_SIZE     = 224\n",
    "# BATCH_SIZE     = 64                        # ‚Üë n·∫øu ƒë·ªß VRAM\n",
    "# EPOCHS         = 10\n",
    "# LR             = 1e-4\n",
    "# WEIGHT_DECAY   = 1e-4\n",
    "# NUM_WORKERS    = min(8, os.cpu_count() or 4)\n",
    "# USE_AMP_WISH   = True\n",
    "# CHANNELS_LAST  = True\n",
    "# AUGMENT        = False                     # False = nhanh h∆°n (gi·ªØ Normalize)\n",
    "# MAX_TRAIN_SAMPLES_PER_EPOCH = 20000        # None = d√πng full d·ªØ li·ªáu m·ªói epoch\n",
    "# FREEZE_BACKBONE = True\n",
    "# WARMUP_EPOCHS   = 2\n",
    "# DROP_CONNECT   = 0.2\n",
    "# DROPOUT        = 0.3\n",
    "# DROP_RATE_XCP  = 0.2\n",
    "# DROP_PATH_XCP  = 0.1\n",
    "\n",
    "# # ---------- Setup ----------\n",
    "# def set_seed(s=42):\n",
    "#     random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "#     torch.cuda.manual_seed_all(s); torch.backends.cudnn.benchmark = True\n",
    "# set_seed(42)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# cc = torch.cuda.get_device_capability() if device.type==\"cuda\" else (0,0)\n",
    "# USE_AMP = bool(USE_AMP_WISH and device.type==\"cuda\" and cc[0] >= 7)  # P100 (6.0) -> False\n",
    "# DATASET_ALIAS = Path(DATASET_ROOT).name\n",
    "# if device.type==\"cuda\":\n",
    "#     try: torch.set_float32_matmul_precision(\"high\")\n",
    "#     except: pass\n",
    "#     print(f\"GPU: {torch.cuda.get_device_name(0)} | CC: {cc[0]}.{cc[1]} | AMP: {USE_AMP}\")\n",
    "\n",
    "# # ---------- Data ----------\n",
    "# mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
    "# eval_tfms = transforms.Compose([\n",
    "#     transforms.Resize(int(INPUT_SIZE*1.15), interpolation=InterpolationMode.BILINEAR),\n",
    "#     transforms.CenterCrop(INPUT_SIZE),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std),\n",
    "# ])\n",
    "# train_tfms = eval_tfms if not AUGMENT else transforms.Compose([\n",
    "#     transforms.Resize(int(INPUT_SIZE*1.15), interpolation=InterpolationMode.BILINEAR),\n",
    "#     transforms.RandomCrop(INPUT_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std),\n",
    "# ])\n",
    "\n",
    "# def make_loader(ds, shuffle, batch_size=BATCH_SIZE):\n",
    "#     kwargs = dict(batch_size=batch_size, shuffle=shuffle,\n",
    "#                   num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"))\n",
    "#     if NUM_WORKERS > 0: kwargs.update(dict(persistent_workers=True, prefetch_factor=4))\n",
    "#     return DataLoader(ds, **kwargs)\n",
    "\n",
    "# # t·∫°o dataset 1 l·∫ßn\n",
    "# train_ds = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"train\"), transform=train_tfms)\n",
    "# val_ds   = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"val\"),   transform=eval_tfms)\n",
    "# test_ds  = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"test\"),  transform=eval_tfms)\n",
    "# print(f\"[train] {len(train_ds)} | [val] {len(val_ds)} | [test] {len(test_ds)} | classes={train_ds.classes}\")\n",
    "# NUM_CLASSES = len(train_ds.classes)\n",
    "\n",
    "# # ---------- Model ----------\n",
    "# from my_efficientnet import EfficientNetB3\n",
    "# from my_xception import Xception\n",
    "\n",
    "# def build_model(key, num_classes):\n",
    "#     k = key.lower()\n",
    "#     if k == \"efficientnet_b3\":\n",
    "#         m = EfficientNetB3(num_classes=num_classes,\n",
    "#                            drop_connect_rate=DROP_CONNECT,\n",
    "#                            dropout=DROPOUT,\n",
    "#                            pretrained=True,\n",
    "#                            freeze_backbone=FREEZE_BACKBONE)\n",
    "#         name = \"efficientnet_b3\"\n",
    "#     elif k == \"xception\":\n",
    "#         m = Xception(num_classes=num_classes,\n",
    "#                      drop_rate=DROP_RATE_XCP,\n",
    "#                      drop_path_rate=DROP_PATH_XCP,\n",
    "#                      pretrained=True,\n",
    "#                      freeze_backbone=FREEZE_BACKBONE)\n",
    "#         name = \"xception\"\n",
    "#     else:\n",
    "#         raise ValueError(k)\n",
    "#     return m, name\n",
    "\n",
    "# model, model_name = build_model(MODEL_KEY, NUM_CLASSES)\n",
    "# model = model.to(device, memory_format=torch.channels_last if CHANNELS_LAST else torch.contiguous_format)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "# scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "# # ---------- Train/Eval ----------\n",
    "# def run_epoch(loader, train_mode=True):\n",
    "#     model.train(train_mode)\n",
    "#     tot, correct, loss_sum = 0, 0, 0.0\n",
    "#     for x,y in loader:\n",
    "#         x = x.to(device, non_blocking=True)\n",
    "#         if CHANNELS_LAST: x = x.to(memory_format=torch.channels_last)\n",
    "#         y = y.to(device, non_blocking=True)\n",
    "#         with torch.set_grad_enabled(train_mode):\n",
    "#             with torch.amp.autocast('cuda', enabled=scaler.is_enabled()):\n",
    "#                 logits = model(x); loss = criterion(logits, y)\n",
    "#         if train_mode:\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             if scaler.is_enabled():\n",
    "#                 scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "#             else:\n",
    "#                 loss.backward(); optimizer.step()\n",
    "#         loss_sum += loss.item()*x.size(0)\n",
    "#         correct += (logits.argmax(1)==y).sum().item()\n",
    "#         tot += x.size(0)\n",
    "#     return loss_sum/tot, correct/tot\n",
    "\n",
    "# # ---------- Loop ----------\n",
    "# best_val = -1.0\n",
    "# ckpt_path = f\"{model_name}_{DATASET_ALIAS}_best.pth\"\n",
    "# val_loader = make_loader(val_ds, shuffle=False)\n",
    "# test_loader = make_loader(test_ds, shuffle=False)\n",
    "\n",
    "# for ep in range(1, EPOCHS+1):\n",
    "#     if FREEZE_BACKBONE and ep == WARMUP_EPOCHS+1 and hasattr(model, \"unfreeze\"):\n",
    "#         model.unfreeze()\n",
    "\n",
    "#     # ch·ªçn ng·∫´u nhi√™n N m·∫´u/epoch (n·∫øu set)\n",
    "#     if MAX_TRAIN_SAMPLES_PER_EPOCH and MAX_TRAIN_SAMPLES_PER_EPOCH < len(train_ds):\n",
    "#         idx = np.random.permutation(len(train_ds))[:MAX_TRAIN_SAMPLES_PER_EPOCH]\n",
    "#         train_sub = Subset(train_ds, idx)\n",
    "#         train_loader = make_loader(train_sub, shuffle=True)\n",
    "#         epoch_info = f\"{len(idx)}/{len(train_ds)} imgs\"\n",
    "#     else:\n",
    "#         train_loader = make_loader(train_ds, shuffle=True)\n",
    "#         epoch_info = f\"{len(train_ds)} imgs\"\n",
    "\n",
    "#     t0 = time.time()\n",
    "#     tr_loss, tr_acc = run_epoch(train_loader, True)\n",
    "#     val_loss, val_acc = run_epoch(val_loader, False)\n",
    "#     scheduler.step()\n",
    "\n",
    "#     if val_acc > best_val:\n",
    "#         best_val = val_acc\n",
    "#         torch.save({\"model\": model.state_dict(),\n",
    "#                     \"epoch\": ep,\n",
    "#                     \"val_acc\": best_val,\n",
    "#                     \"model_name\": model_name,\n",
    "#                     \"dataset_alias\": DATASET_ALIAS,\n",
    "#                     \"input_size\": INPUT_SIZE}, ckpt_path)\n",
    "\n",
    "#     print(f\"Epoch {ep:02d} [{epoch_info}] | \"\n",
    "#           f\"train {tr_loss:.4f}/{tr_acc:.4f} | \"\n",
    "#           f\"val {val_loss:.4f}/{val_acc:.4f} | \"\n",
    "#           f\"best {best_val:.4f} | time {time.time()-t0:.1f}s\")\n",
    "\n",
    "# print(f\"\\nSaved: {ckpt_path}\")\n",
    "\n",
    "# # ---------- Test ----------\n",
    "# sd = torch.load(ckpt_path, map_location=device)[\"model\"]\n",
    "# model.load_state_dict(sd)\n",
    "# test_loss, test_acc = run_epoch(test_loader, False)\n",
    "# print(f\"TEST: loss {test_loss:.4f} | acc {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ae602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FAST Fine-tune: EfficientNetB3 / Xception v·ªõi Augmentation kh√°c nhau =====\n",
    "import os, time, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# ---------- Model-specific Config (CH·ªà ƒê·ªîI AUGMENTATION) ----------\n",
    "CONFIG = {\n",
    "    \"xception\": {\n",
    "        \"augment\": \"strong\",      # Strong augmentation (RandAugment + RandomErasing)\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 1e-4,\n",
    "    },\n",
    "    \"efficientnet_b3\": {\n",
    "        \"augment\": \"basic\",       # Basic augmentation (Crop + Flip)\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 1e-4,\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---------- Params ----------\n",
    "DATASET_ROOT   = \"../Dataset\"\n",
    "MODEL_KEY      = \"xception\"                    # {\"efficientnet_b3\", \"xception\"}\n",
    "INPUT_SIZE     = 224\n",
    "BATCH_SIZE     = 64\n",
    "EPOCHS         = 10\n",
    "NUM_WORKERS    = min(8, os.cpu_count() or 4)\n",
    "USE_AMP_WISH   = True\n",
    "CHANNELS_LAST  = True\n",
    "MAX_TRAIN_SAMPLES_PER_EPOCH = 20000\n",
    "FREEZE_BACKBONE = True\n",
    "WARMUP_EPOCHS   = 2\n",
    "DROP_CONNECT   = 0.2\n",
    "DROPOUT        = 0.3\n",
    "DROP_RATE_XCP  = 0.2\n",
    "DROP_PATH_XCP  = 0.1\n",
    "\n",
    "# L·∫•y config cho model hi·ªán t·∫°i\n",
    "cfg = CONFIG[MODEL_KEY.lower()]\n",
    "LR = cfg[\"lr\"]\n",
    "WEIGHT_DECAY = cfg[\"weight_decay\"]\n",
    "\n",
    "# ---------- Setup ----------\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s); torch.backends.cudnn.benchmark = True\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cc = torch.cuda.get_device_capability() if device.type==\"cuda\" else (0,0)\n",
    "USE_AMP = bool(USE_AMP_WISH and device.type==\"cuda\" and cc[0] >= 7)\n",
    "DATASET_ALIAS = Path(DATASET_ROOT).name\n",
    "if device.type==\"cuda\":\n",
    "    try: torch.set_float32_matmul_precision(\"high\")\n",
    "    except: pass\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} | CC: {cc[0]}.{cc[1]} | AMP: {USE_AMP}\")\n",
    "\n",
    "print(f\"\\n[Model: {MODEL_KEY}]\")\n",
    "print(f\"Config: {cfg}\")\n",
    "\n",
    "# ---------- Data v·ªõi Augmentation theo config ----------\n",
    "mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(INPUT_SIZE*1.15), interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(INPUT_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "def build_train_transform(augment_type):\n",
    "    base_resize = transforms.Resize(int(INPUT_SIZE*1.15), interpolation=InterpolationMode.BILINEAR)\n",
    "    \n",
    "    if augment_type == \"strong\":\n",
    "        # Strong augmentation: RandAugment + RandomErasing\n",
    "        return transforms.Compose([\n",
    "            base_resize,\n",
    "            transforms.RandomCrop(INPUT_SIZE),\n",
    "            transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            transforms.RandomErasing(p=0.25, scale=(0.02, 0.2)),\n",
    "        ])\n",
    "    else:  # basic\n",
    "        # Basic augmentation: Crop + Flip\n",
    "        return transforms.Compose([\n",
    "            base_resize,\n",
    "            transforms.RandomCrop(INPUT_SIZE),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "train_tfms = build_train_transform(cfg[\"augment\"])\n",
    "\n",
    "def make_loader(ds, shuffle, batch_size=BATCH_SIZE):\n",
    "    kwargs = dict(batch_size=batch_size, shuffle=shuffle,\n",
    "                  num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"))\n",
    "    if NUM_WORKERS > 0: kwargs.update(dict(persistent_workers=True, prefetch_factor=4))\n",
    "    return DataLoader(ds, **kwargs)\n",
    "\n",
    "# t·∫°o dataset\n",
    "train_ds = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"train\"), transform=train_tfms)\n",
    "val_ds   = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"val\"),   transform=eval_tfms)\n",
    "test_ds  = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"test\"),  transform=eval_tfms)\n",
    "print(f\"[train] {len(train_ds)} | [val] {len(val_ds)} | [test] {len(test_ds)} | classes={train_ds.classes}\")\n",
    "NUM_CLASSES = len(train_ds.classes)\n",
    "\n",
    "# ---------- Model ----------\n",
    "from my_efficientnet import EfficientNetB3\n",
    "from my_xception import Xception\n",
    "\n",
    "def build_model(key, num_classes):\n",
    "    k = key.lower()\n",
    "    if k == \"efficientnet_b3\":\n",
    "        m = EfficientNetB3(num_classes=num_classes,\n",
    "                           drop_connect_rate=DROP_CONNECT,\n",
    "                           dropout=DROPOUT,\n",
    "                           pretrained=True,\n",
    "                           freeze_backbone=FREEZE_BACKBONE)\n",
    "        name = \"efficientnet_b3\"\n",
    "    elif k == \"xception\":\n",
    "        m = Xception(num_classes=num_classes,\n",
    "                     drop_rate=DROP_RATE_XCP,\n",
    "                     drop_path_rate=DROP_PATH_XCP,\n",
    "                     pretrained=True,\n",
    "                     freeze_backbone=FREEZE_BACKBONE)\n",
    "        name = \"xception\"\n",
    "    else:\n",
    "        raise ValueError(k)\n",
    "    return m, name\n",
    "\n",
    "model, model_name = build_model(MODEL_KEY, NUM_CLASSES)\n",
    "model = model.to(device, memory_format=torch.channels_last if CHANNELS_LAST else torch.contiguous_format)\n",
    "\n",
    "# ---------- Optimizer & Scheduler (GI·ªêNG NHAU cho c·∫£ 2 model) ----------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "print(f\"Optimizer: AdamW | Scheduler: CosineAnnealingLR | Augment: {cfg['augment']}\")\n",
    "\n",
    "# ---------- Train/Eval ----------\n",
    "def run_epoch(loader, train_mode=True, compute_metrics=False):\n",
    "    model.train(train_mode)\n",
    "    tot, correct, loss_sum = 0, 0, 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    for x,y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        if CHANNELS_LAST: x = x.to(memory_format=torch.channels_last)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            with torch.amp.autocast('cuda', enabled=scaler.is_enabled()):\n",
    "                logits = model(x); loss = criterion(logits, y)\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                loss.backward(); optimizer.step()\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        loss_sum += loss.item()*x.size(0)\n",
    "        correct += (preds==y).sum().item()\n",
    "        tot += x.size(0)\n",
    "        \n",
    "        if compute_metrics:\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    acc = correct/tot\n",
    "    if compute_metrics:\n",
    "        prec = precision_score(y_true, y_pred, average='weighted')\n",
    "        rec = recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        return loss_sum/tot, acc, prec, rec, f1\n",
    "    return loss_sum/tot, acc\n",
    "\n",
    "# ---------- Loop ----------\n",
    "best_val = -1.0\n",
    "ckpt_path = f\"{model_name}_{DATASET_ALIAS}_best.pth\"\n",
    "val_loader = make_loader(val_ds, shuffle=False)\n",
    "test_loader = make_loader(test_ds, shuffle=False)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    if FREEZE_BACKBONE and ep == WARMUP_EPOCHS+1 and hasattr(model, \"unfreeze\"):\n",
    "        model.unfreeze()\n",
    "\n",
    "    # ch·ªçn ng·∫´u nhi√™n N m·∫´u/epoch (n·∫øu set)\n",
    "    if MAX_TRAIN_SAMPLES_PER_EPOCH and MAX_TRAIN_SAMPLES_PER_EPOCH < len(train_ds):\n",
    "        idx = np.random.permutation(len(train_ds))[:MAX_TRAIN_SAMPLES_PER_EPOCH]\n",
    "        train_sub = Subset(train_ds, idx)\n",
    "        train_loader = make_loader(train_sub, shuffle=True)\n",
    "        epoch_info = f\"{len(idx)}/{len(train_ds)} imgs\"\n",
    "    else:\n",
    "        train_loader = make_loader(train_ds, shuffle=True)\n",
    "        epoch_info = f\"{len(train_ds)} imgs\"\n",
    "\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, True)\n",
    "    val_loss, val_acc = run_epoch(val_loader, False)\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save({\"model\": model.state_dict(),\n",
    "                    \"epoch\": ep,\n",
    "                    \"val_acc\": best_val,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"dataset_alias\": DATASET_ALIAS,\n",
    "                    \"input_size\": INPUT_SIZE,\n",
    "                    \"config\": cfg}, ckpt_path)\n",
    "\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {ep:02d} [{epoch_info}] | \"\n",
    "          f\"train {tr_loss:.4f}/{tr_acc:.4f} | \"\n",
    "          f\"val {val_loss:.4f}/{val_acc:.4f} | \"\n",
    "          f\"best {best_val:.4f} | lr {curr_lr:.2e} | time {time.time()-t0:.1f}s\")\n",
    "\n",
    "print(f\"\\nSaved: {ckpt_path}\")\n",
    "\n",
    "# ---------- Test v·ªõi checkpoint ƒë√£ l∆∞u ----------\n",
    "print(\"\\nüìÇ Loading best checkpoint for testing...\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "test_loss, test_acc, prec, rec, f1 = run_epoch(test_loader, False, compute_metrics=True)\n",
    "print(f\"TEST: loss {test_loss:.4f} | acc {test_acc:.4f} | prec {prec:.4f} | rec {rec:.4f} | f1 {f1:.4f}\")\n",
    "\n",
    "# C·∫≠p nh·∫≠t checkpoint v·ªõi test_acc\n",
    "checkpoint['test_acc'] = test_acc\n",
    "torch.save(checkpoint, ckpt_path)\n",
    "print(f\"‚úÖ Updated {ckpt_path} with test_acc: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
