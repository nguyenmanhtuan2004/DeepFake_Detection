{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965d31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] classes -> ['fake', 'real'] {'fake': 0, 'real': 1}\n",
      "[val] classes -> ['fake', 'real'] {'fake': 0, 'real': 1}\n",
      "[test] classes -> ['fake', 'real'] {'fake': 0, 'real': 1}\n",
      "[EfficientNetB3] Đang tạo model 'tf_efficientnet_b3' với pretrained=True\n",
      "[EfficientNetB3] ✅ Tạo thành công!\n",
      "[EfficientNetB3] ✅ Model validation passed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munfreeze\u001b[39m\u001b[38;5;124m\"\u001b[39m): model\u001b[38;5;241m.\u001b[39munfreeze()\n\u001b[0;32m    126\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 127\u001b[0m tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m run_epoch(train_loader, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    128\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m run_epoch(val_loader, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    129\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[4], line 107\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, train_mode)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(train_mode):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, enabled\u001b[38;5;241m=\u001b[39mscaler\u001b[38;5;241m.\u001b[39mis_enabled()):\n\u001b[1;32m--> 107\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(x); loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[0;32m    109\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\ComputerVision\\LYTHUYET\\DeepFake_Detection\\my_efficientnet.py:166\u001b[0m, in \u001b[0;36mEfficientNetB3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# DataParallel-safe forward: trực tiếp gọi backbone thay vì check hasattr\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# hasattr có thể fail khi model được replicate sang GPU khác\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(x)\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    169\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEfficientNetB3.backbone không tồn tại. Có thể do:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Lỗi trong __init__ (backbone chưa được tạo)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. DataParallel replication issues\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Model được load không đúng cách\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\timm\\models\\efficientnet.py:339\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_features(x)\n\u001b[0;32m    340\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\timm\\models\\efficientnet.py:317\u001b[0m, in \u001b[0;36mEfficientNet.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    315\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x, flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n\u001b[0;32m    318\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_head(x)\n\u001b[0;32m    319\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\timm\\models\\_efficientnet_blocks.py:289\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    287\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    288\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_dw(x)\n\u001b[1;32m--> 289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x)\n\u001b[0;32m    290\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maa(x)\n\u001b[0;32m    291\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mse(x)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\timm\\layers\\norm_act.py:148\u001b[0m, in \u001b[0;36mBatchNormAct2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m    137\u001b[0m     x,\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    146\u001b[0m )\n\u001b[0;32m    147\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[1;32m--> 148\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:434\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2374\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   2373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   2375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # ===== FAST Fine-tune: EfficientNetB3 / Xception =====\n",
    "# import os, time, random\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "# from torchvision import datasets, transforms\n",
    "# from torchvision.transforms import InterpolationMode\n",
    "\n",
    "# # ---------- Params ----------\n",
    "# DATASET_ROOT   = \"../Dataset\"                 # Dataset/train, val, test\n",
    "# MODEL_KEY      = \"efficientnet_b3\"         # {\"efficientnet_b3\", \"xception\"}\n",
    "# INPUT_SIZE     = 224\n",
    "# BATCH_SIZE     = 64                        # ↑ nếu đủ VRAM\n",
    "# EPOCHS         = 10\n",
    "# LR             = 1e-4\n",
    "# WEIGHT_DECAY   = 1e-4\n",
    "# NUM_WORKERS    = min(8, os.cpu_count() or 4)\n",
    "# USE_AMP_WISH   = True\n",
    "# CHANNELS_LAST  = True\n",
    "# AUGMENT        = False                     # False = nhanh hơn (giữ Normalize)\n",
    "# MAX_TRAIN_SAMPLES_PER_EPOCH = 20000        # None = dùng full dữ liệu mỗi epoch\n",
    "# FREEZE_BACKBONE = True\n",
    "# WARMUP_EPOCHS   = 2\n",
    "# DROP_CONNECT   = 0.2\n",
    "# DROPOUT        = 0.3\n",
    "# DROP_RATE_XCP  = 0.2\n",
    "# DROP_PATH_XCP  = 0.1\n",
    "\n",
    "# # ---------- Setup ----------\n",
    "# def set_seed(s=42):\n",
    "#     random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "#     torch.cuda.manual_seed_all(s); torch.backends.cudnn.benchmark = True\n",
    "# set_seed(42)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# cc = torch.cuda.get_device_capability() if device.type==\"cuda\" else (0,0)\n",
    "# USE_AMP = bool(USE_AMP_WISH and device.type==\"cuda\" and cc[0] >= 7)  # P100 (6.0) -> False\n",
    "# DATASET_ALIAS = Path(DATASET_ROOT).name\n",
    "# if device.type==\"cuda\":\n",
    "#     try: torch.set_float32_matmul_precision(\"high\")\n",
    "#     except: pass\n",
    "#     print(f\"GPU: {torch.cuda.get_device_name(0)} | CC: {cc[0]}.{cc[1]} | AMP: {USE_AMP}\")\n",
    "\n",
    "# # ---------- Data ----------\n",
    "# mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
    "# eval_tfms = transforms.Compose([\n",
    "#     transforms.Resize(int(INPUT_SIZE*1.15), interpolation=InterpolationMode.BILINEAR),\n",
    "#     transforms.CenterCrop(INPUT_SIZE),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std),\n",
    "# ])\n",
    "# train_tfms = eval_tfms if not AUGMENT else transforms.Compose([\n",
    "#     transforms.Resize(int(INPUT_SIZE*1.15), interpolation=InterpolationMode.BILINEAR),\n",
    "#     transforms.RandomCrop(INPUT_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std),\n",
    "# ])\n",
    "\n",
    "# def make_loader(ds, shuffle, batch_size=BATCH_SIZE):\n",
    "#     kwargs = dict(batch_size=batch_size, shuffle=shuffle,\n",
    "#                   num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"))\n",
    "#     if NUM_WORKERS > 0: kwargs.update(dict(persistent_workers=True, prefetch_factor=4))\n",
    "#     return DataLoader(ds, **kwargs)\n",
    "\n",
    "# # tạo dataset 1 lần\n",
    "# train_ds = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"train\"), transform=train_tfms)\n",
    "# val_ds   = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"val\"),   transform=eval_tfms)\n",
    "# test_ds  = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"test\"),  transform=eval_tfms)\n",
    "# print(f\"[train] {len(train_ds)} | [val] {len(val_ds)} | [test] {len(test_ds)} | classes={train_ds.classes}\")\n",
    "# NUM_CLASSES = len(train_ds.classes)\n",
    "\n",
    "# # ---------- Model ----------\n",
    "# from my_efficientnet import EfficientNetB3\n",
    "# from my_xception import Xception\n",
    "\n",
    "# def build_model(key, num_classes):\n",
    "#     k = key.lower()\n",
    "#     if k == \"efficientnet_b3\":\n",
    "#         m = EfficientNetB3(num_classes=num_classes,\n",
    "#                            drop_connect_rate=DROP_CONNECT,\n",
    "#                            dropout=DROPOUT,\n",
    "#                            pretrained=True,\n",
    "#                            freeze_backbone=FREEZE_BACKBONE)\n",
    "#         name = \"efficientnet_b3\"\n",
    "#     elif k == \"xception\":\n",
    "#         m = Xception(num_classes=num_classes,\n",
    "#                      drop_rate=DROP_RATE_XCP,\n",
    "#                      drop_path_rate=DROP_PATH_XCP,\n",
    "#                      pretrained=True,\n",
    "#                      freeze_backbone=FREEZE_BACKBONE)\n",
    "#         name = \"xception\"\n",
    "#     else:\n",
    "#         raise ValueError(k)\n",
    "#     return m, name\n",
    "\n",
    "# model, model_name = build_model(MODEL_KEY, NUM_CLASSES)\n",
    "# model = model.to(device, memory_format=torch.channels_last if CHANNELS_LAST else torch.contiguous_format)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "# scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "# # ---------- Train/Eval ----------\n",
    "# def run_epoch(loader, train_mode=True):\n",
    "#     model.train(train_mode)\n",
    "#     tot, correct, loss_sum = 0, 0, 0.0\n",
    "#     for x,y in loader:\n",
    "#         x = x.to(device, non_blocking=True)\n",
    "#         if CHANNELS_LAST: x = x.to(memory_format=torch.channels_last)\n",
    "#         y = y.to(device, non_blocking=True)\n",
    "#         with torch.set_grad_enabled(train_mode):\n",
    "#             with torch.amp.autocast('cuda', enabled=scaler.is_enabled()):\n",
    "#                 logits = model(x); loss = criterion(logits, y)\n",
    "#         if train_mode:\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             if scaler.is_enabled():\n",
    "#                 scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "#             else:\n",
    "#                 loss.backward(); optimizer.step()\n",
    "#         loss_sum += loss.item()*x.size(0)\n",
    "#         correct += (logits.argmax(1)==y).sum().item()\n",
    "#         tot += x.size(0)\n",
    "#     return loss_sum/tot, correct/tot\n",
    "\n",
    "# # ---------- Loop ----------\n",
    "# best_val = -1.0\n",
    "# ckpt_path = f\"{model_name}_{DATASET_ALIAS}_best.pth\"\n",
    "# val_loader = make_loader(val_ds, shuffle=False)\n",
    "# test_loader = make_loader(test_ds, shuffle=False)\n",
    "\n",
    "# for ep in range(1, EPOCHS+1):\n",
    "#     if FREEZE_BACKBONE and ep == WARMUP_EPOCHS+1 and hasattr(model, \"unfreeze\"):\n",
    "#         model.unfreeze()\n",
    "\n",
    "#     # chọn ngẫu nhiên N mẫu/epoch (nếu set)\n",
    "#     if MAX_TRAIN_SAMPLES_PER_EPOCH and MAX_TRAIN_SAMPLES_PER_EPOCH < len(train_ds):\n",
    "#         idx = np.random.permutation(len(train_ds))[:MAX_TRAIN_SAMPLES_PER_EPOCH]\n",
    "#         train_sub = Subset(train_ds, idx)\n",
    "#         train_loader = make_loader(train_sub, shuffle=True)\n",
    "#         epoch_info = f\"{len(idx)}/{len(train_ds)} imgs\"\n",
    "#     else:\n",
    "#         train_loader = make_loader(train_ds, shuffle=True)\n",
    "#         epoch_info = f\"{len(train_ds)} imgs\"\n",
    "\n",
    "#     t0 = time.time()\n",
    "#     tr_loss, tr_acc = run_epoch(train_loader, True)\n",
    "#     val_loss, val_acc = run_epoch(val_loader, False)\n",
    "#     scheduler.step()\n",
    "\n",
    "#     if val_acc > best_val:\n",
    "#         best_val = val_acc\n",
    "#         torch.save({\"model\": model.state_dict(),\n",
    "#                     \"epoch\": ep,\n",
    "#                     \"val_acc\": best_val,\n",
    "#                     \"model_name\": model_name,\n",
    "#                     \"dataset_alias\": DATASET_ALIAS,\n",
    "#                     \"input_size\": INPUT_SIZE}, ckpt_path)\n",
    "\n",
    "#     print(f\"Epoch {ep:02d} [{epoch_info}] | \"\n",
    "#           f\"train {tr_loss:.4f}/{tr_acc:.4f} | \"\n",
    "#           f\"val {val_loss:.4f}/{val_acc:.4f} | \"\n",
    "#           f\"best {best_val:.4f} | time {time.time()-t0:.1f}s\")\n",
    "\n",
    "# print(f\"\\nSaved: {ckpt_path}\")\n",
    "\n",
    "# # ---------- Test ----------\n",
    "# sd = torch.load(ckpt_path, map_location=device)[\"model\"]\n",
    "# model.load_state_dict(sd)\n",
    "# test_loss, test_acc = run_epoch(test_loader, False)\n",
    "# print(f\"TEST: loss {test_loss:.4f} | acc {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ae602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FAST Fine-tune: EfficientNetB3 / Xception với Augmentation khác nhau =====\n",
    "import os, time, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# ---------- Model-specific Config (CHỈ ĐỔI AUGMENTATION) ----------\n",
    "CONFIG = {\n",
    "    \"xception\": {\n",
    "        \"augment\": \"strong\",      # Strong augmentation (RandAugment + RandomErasing)\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 1e-4,\n",
    "    },\n",
    "    \"efficientnet_b3\": {\n",
    "        \"augment\": \"basic\",       # Basic augmentation (Crop + Flip)\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 1e-4,\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---------- Params ----------\n",
    "DATASET_ROOT   = \"../Dataset\"\n",
    "MODEL_KEY      = \"xception\"                    # {\"efficientnet_b3\", \"xception\"}\n",
    "INPUT_SIZE     = 224\n",
    "BATCH_SIZE     = 64\n",
    "EPOCHS         = 10\n",
    "NUM_WORKERS    = min(8, os.cpu_count() or 4)\n",
    "USE_AMP_WISH   = True\n",
    "CHANNELS_LAST  = True\n",
    "MAX_TRAIN_SAMPLES_PER_EPOCH = 20000\n",
    "FREEZE_BACKBONE = True\n",
    "WARMUP_EPOCHS   = 2\n",
    "DROP_CONNECT   = 0.2\n",
    "DROPOUT        = 0.3\n",
    "DROP_RATE_XCP  = 0.2\n",
    "DROP_PATH_XCP  = 0.1\n",
    "\n",
    "# Lấy config cho model hiện tại\n",
    "cfg = CONFIG[MODEL_KEY.lower()]\n",
    "LR = cfg[\"lr\"]\n",
    "WEIGHT_DECAY = cfg[\"weight_decay\"]\n",
    "\n",
    "# ---------- Setup ----------\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s); torch.backends.cudnn.benchmark = True\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cc = torch.cuda.get_device_capability() if device.type==\"cuda\" else (0,0)\n",
    "USE_AMP = bool(USE_AMP_WISH and device.type==\"cuda\" and cc[0] >= 7)\n",
    "DATASET_ALIAS = Path(DATASET_ROOT).name\n",
    "if device.type==\"cuda\":\n",
    "    try: torch.set_float32_matmul_precision(\"high\")\n",
    "    except: pass\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} | CC: {cc[0]}.{cc[1]} | AMP: {USE_AMP}\")\n",
    "\n",
    "print(f\"\\n[Model: {MODEL_KEY}]\")\n",
    "print(f\"Config: {cfg}\")\n",
    "\n",
    "# ---------- Data với Augmentation theo config ----------\n",
    "mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(INPUT_SIZE*1.15), interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(INPUT_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "def build_train_transform(augment_type):\n",
    "    base_resize = transforms.Resize(int(INPUT_SIZE*1.15), interpolation=InterpolationMode.BILINEAR)\n",
    "    \n",
    "    if augment_type == \"strong\":\n",
    "        # Strong augmentation: RandAugment + RandomErasing\n",
    "        return transforms.Compose([\n",
    "            base_resize,\n",
    "            transforms.RandomCrop(INPUT_SIZE),\n",
    "            transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            transforms.RandomErasing(p=0.25, scale=(0.02, 0.2)),\n",
    "        ])\n",
    "    else:  # basic\n",
    "        # Basic augmentation: Crop + Flip\n",
    "        return transforms.Compose([\n",
    "            base_resize,\n",
    "            transforms.RandomCrop(INPUT_SIZE),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "train_tfms = build_train_transform(cfg[\"augment\"])\n",
    "\n",
    "def make_loader(ds, shuffle, batch_size=BATCH_SIZE):\n",
    "    kwargs = dict(batch_size=batch_size, shuffle=shuffle,\n",
    "                  num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"))\n",
    "    if NUM_WORKERS > 0: kwargs.update(dict(persistent_workers=True, prefetch_factor=4))\n",
    "    return DataLoader(ds, **kwargs)\n",
    "\n",
    "# tạo dataset\n",
    "train_ds = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"train\"), transform=train_tfms)\n",
    "val_ds   = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"val\"),   transform=eval_tfms)\n",
    "test_ds  = datasets.ImageFolder(os.path.join(DATASET_ROOT, \"test\"),  transform=eval_tfms)\n",
    "print(f\"[train] {len(train_ds)} | [val] {len(val_ds)} | [test] {len(test_ds)} | classes={train_ds.classes}\")\n",
    "NUM_CLASSES = len(train_ds.classes)\n",
    "\n",
    "# ---------- Model ----------\n",
    "from my_efficientnet import EfficientNetB3\n",
    "from my_xception import Xception\n",
    "\n",
    "def build_model(key, num_classes):\n",
    "    k = key.lower()\n",
    "    if k == \"efficientnet_b3\":\n",
    "        m = EfficientNetB3(num_classes=num_classes,\n",
    "                           drop_connect_rate=DROP_CONNECT,\n",
    "                           dropout=DROPOUT,\n",
    "                           pretrained=True,\n",
    "                           freeze_backbone=FREEZE_BACKBONE)\n",
    "        name = \"efficientnet_b3\"\n",
    "    elif k == \"xception\":\n",
    "        m = Xception(num_classes=num_classes,\n",
    "                     drop_rate=DROP_RATE_XCP,\n",
    "                     drop_path_rate=DROP_PATH_XCP,\n",
    "                     pretrained=True,\n",
    "                     freeze_backbone=FREEZE_BACKBONE)\n",
    "        name = \"xception\"\n",
    "    else:\n",
    "        raise ValueError(k)\n",
    "    return m, name\n",
    "\n",
    "model, model_name = build_model(MODEL_KEY, NUM_CLASSES)\n",
    "model = model.to(device, memory_format=torch.channels_last if CHANNELS_LAST else torch.contiguous_format)\n",
    "\n",
    "# ---------- Optimizer & Scheduler (GIỐNG NHAU cho cả 2 model) ----------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "print(f\"Optimizer: AdamW | Scheduler: CosineAnnealingLR | Augment: {cfg['augment']}\")\n",
    "\n",
    "# ---------- Train/Eval ----------\n",
    "def run_epoch(loader, train_mode=True, compute_metrics=False):\n",
    "    model.train(train_mode)\n",
    "    tot, correct, loss_sum = 0, 0, 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    for x,y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        if CHANNELS_LAST: x = x.to(memory_format=torch.channels_last)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            with torch.amp.autocast('cuda', enabled=scaler.is_enabled()):\n",
    "                logits = model(x); loss = criterion(logits, y)\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                loss.backward(); optimizer.step()\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        loss_sum += loss.item()*x.size(0)\n",
    "        correct += (preds==y).sum().item()\n",
    "        tot += x.size(0)\n",
    "        \n",
    "        if compute_metrics:\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    acc = correct/tot\n",
    "    if compute_metrics:\n",
    "        prec = precision_score(y_true, y_pred, average='weighted')\n",
    "        rec = recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        return loss_sum/tot, acc, prec, rec, f1\n",
    "    return loss_sum/tot, acc\n",
    "\n",
    "# ---------- Loop ----------\n",
    "best_val = -1.0\n",
    "ckpt_path = f\"{model_name}_{DATASET_ALIAS}_best.pth\"\n",
    "val_loader = make_loader(val_ds, shuffle=False)\n",
    "test_loader = make_loader(test_ds, shuffle=False)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    if FREEZE_BACKBONE and ep == WARMUP_EPOCHS+1 and hasattr(model, \"unfreeze\"):\n",
    "        model.unfreeze()\n",
    "\n",
    "    # chọn ngẫu nhiên N mẫu/epoch (nếu set)\n",
    "    if MAX_TRAIN_SAMPLES_PER_EPOCH and MAX_TRAIN_SAMPLES_PER_EPOCH < len(train_ds):\n",
    "        idx = np.random.permutation(len(train_ds))[:MAX_TRAIN_SAMPLES_PER_EPOCH]\n",
    "        train_sub = Subset(train_ds, idx)\n",
    "        train_loader = make_loader(train_sub, shuffle=True)\n",
    "        epoch_info = f\"{len(idx)}/{len(train_ds)} imgs\"\n",
    "    else:\n",
    "        train_loader = make_loader(train_ds, shuffle=True)\n",
    "        epoch_info = f\"{len(train_ds)} imgs\"\n",
    "\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, True)\n",
    "    val_loss, val_acc = run_epoch(val_loader, False)\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save({\"model\": model.state_dict(),\n",
    "                    \"epoch\": ep,\n",
    "                    \"val_acc\": best_val,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"dataset_alias\": DATASET_ALIAS,\n",
    "                    \"input_size\": INPUT_SIZE,\n",
    "                    \"config\": cfg}, ckpt_path)\n",
    "\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {ep:02d} [{epoch_info}] | \"\n",
    "          f\"train {tr_loss:.4f}/{tr_acc:.4f} | \"\n",
    "          f\"val {val_loss:.4f}/{val_acc:.4f} | \"\n",
    "          f\"best {best_val:.4f} | lr {curr_lr:.2e} | time {time.time()-t0:.1f}s\")\n",
    "\n",
    "print(f\"\\nSaved: {ckpt_path}\")\n",
    "\n",
    "# ---------- Test với checkpoint đã lưu ----------\n",
    "print(\"\\n📂 Loading best checkpoint for testing...\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "print(\"✅ Model loaded!\")\n",
    "\n",
    "test_loss, test_acc, prec, rec, f1 = run_epoch(test_loader, False, compute_metrics=True)\n",
    "print(f\"TEST: loss {test_loss:.4f} | acc {test_acc:.4f} | prec {prec:.4f} | rec {rec:.4f} | f1 {f1:.4f}\")\n",
    "\n",
    "# Cập nhật checkpoint với test_acc\n",
    "checkpoint['test_acc'] = test_acc\n",
    "torch.save(checkpoint, ckpt_path)\n",
    "print(f\"✅ Updated {ckpt_path} with test_acc: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
